---
layout: jupyter
title: 자연어 처리 - 간단한 RNN 예제
published: true
date: 2023-01-12
description: 간단한 RNN 예제
comments: true
categories:
 - NLP
tags:
 - python
 - NLP
toc: true
toc_sticky: true
toc_label: 간단한 RNN 예제
use_math: true
---
---
# 간단한 RNN 예제

RNN(Recurrent Neural Network)

<div class="in_prompt">
In&nbsp;[1]:
</div>

<div class="input_area" markdown="1">

```python
text = '''저 별을 따다가 니 귀에 걸어주고파
저 달 따다가 니 목에 걸어주고파
세상 모든 좋은 것만 해주고 싶은
이런 내 맘을 그댄 아나요'''
```

</div>

저? -> 별을  
저 별을? -> 따다가  
저 별을 따다가? -> 니  
저 별을 따다가 니 귀에? -> 걸어주고파

    다음에 올 단어를 예측! -> RNN

## 데이터 전처리

<div class="in_prompt">
In&nbsp;[2]:
</div>

<div class="input_area" markdown="1">

```python
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical, pad_sequences
```

</div>

### 단어 정수화

<div class="in_prompt">
In&nbsp;[3]:
</div>

<div class="input_area" markdown="1">

```python
tok = Tokenizer()
tok.fit_on_texts([text])
tok.word_index
```

</div>

<div class="output_prompt">
Out&nbsp;[3]:
</div>




{:.output_data_text}

```
{'저': 1,
 '따다가': 2,
 '니': 3,
 '걸어주고파': 4,
 '별을': 5,
 '귀에': 6,
 '달': 7,
 '목에': 8,
 '세상': 9,
 '모든': 10,
 '좋은': 11,
 '것만': 12,
 '해주고': 13,
 '싶은': 14,
 '이런': 15,
 '내': 16,
 '맘을': 17,
 '그댄': 18,
 '아나요': 19}
```



<div class="in_prompt">
In&nbsp;[4]:
</div>

<div class="input_area" markdown="1">

```python
vocab_size = len(tok.word_index) + 1 # zero padding용 공간 추가
vocab_size
```

</div>

<div class="output_prompt">
Out&nbsp;[4]:
</div>




{:.output_data_text}

```
20
```



### 문단 정수화

<div class="in_prompt">
In&nbsp;[5]:
</div>

<div class="input_area" markdown="1">

```python
for sentence in text.split('\n'):
    res = tok.texts_to_sequences([sentence])[0]
    print(res)
```

</div>

<div class="output_prompt">
Out&nbsp;[5]:
</div>

{:.output_stream}

```
[1, 5, 2, 3, 6, 4]
[1, 7, 2, 3, 8, 4]
[9, 10, 11, 12, 13, 14]
[15, 16, 17, 18, 19]

```

저? -> 별을  
저 별을? -> 따다가  
저 별을 따다가? -> 니  
저 별을 따다가 니 귀에? -> 걸어주고파  

    다음에 올 단어를 예측! -> RNN

<div class="in_prompt">
In&nbsp;[6]:
</div>

<div class="input_area" markdown="1">

```python
# 다음 단어를 유추할 수 있게 확장
seq_list = []
for sentence in text.split('\n'):
    res = tok.texts_to_sequences([sentence])[0]
    for i in range(1, len(res)):
        seq = res[:i+1]
        seq_list.append(seq)
seq_list
```

</div>

<div class="output_prompt">
Out&nbsp;[6]:
</div>




{:.output_data_text}

```
[[1, 5],
 [1, 5, 2],
 [1, 5, 2, 3],
 [1, 5, 2, 3, 6],
 [1, 5, 2, 3, 6, 4],
 [1, 7],
 [1, 7, 2],
 [1, 7, 2, 3],
 [1, 7, 2, 3, 8],
 [1, 7, 2, 3, 8, 4],
 [9, 10],
 [9, 10, 11],
 [9, 10, 11, 12],
 [9, 10, 11, 12, 13],
 [9, 10, 11, 12, 13, 14],
 [15, 16],
 [15, 16, 17],
 [15, 16, 17, 18],
 [15, 16, 17, 18, 19]]
```



<div class="in_prompt">
In&nbsp;[7]:
</div>

<div class="input_area" markdown="1">

```python
# ?
max_len = max(len(sent) for sent in seq_list)
max_len
```

</div>

<div class="output_prompt">
Out&nbsp;[7]:
</div>




{:.output_data_text}

```
6
```



### zero padding

<div class="in_prompt">
In&nbsp;[8]:
</div>

<div class="input_area" markdown="1">

```python
seq_padded = pad_sequences(seq_list, maxlen = max_len)
seq_padded
```

</div>

<div class="output_prompt">
Out&nbsp;[8]:
</div>




{:.output_data_text}

```
array([[ 0,  0,  0,  0,  1,  5],
       [ 0,  0,  0,  1,  5,  2],
       [ 0,  0,  1,  5,  2,  3],
       [ 0,  1,  5,  2,  3,  6],
       [ 1,  5,  2,  3,  6,  4],
       [ 0,  0,  0,  0,  1,  7],
       [ 0,  0,  0,  1,  7,  2],
       [ 0,  0,  1,  7,  2,  3],
       [ 0,  1,  7,  2,  3,  8],
       [ 1,  7,  2,  3,  8,  4],
       [ 0,  0,  0,  0,  9, 10],
       [ 0,  0,  0,  9, 10, 11],
       [ 0,  0,  9, 10, 11, 12],
       [ 0,  9, 10, 11, 12, 13],
       [ 9, 10, 11, 12, 13, 14],
       [ 0,  0,  0,  0, 15, 16],
       [ 0,  0,  0, 15, 16, 17],
       [ 0,  0, 15, 16, 17, 18],
       [ 0, 15, 16, 17, 18, 19]])
```



<div class="in_prompt">
In&nbsp;[9]:
</div>

<div class="input_area" markdown="1">

```python
# 앞의 단어들(X)이 오면 뒤에 올 단어(y)가 온다고 학습
X = seq_padded[:, :-1]
y = seq_padded[:, -1]
X, y
```

</div>

<div class="output_prompt">
Out&nbsp;[9]:
</div>




{:.output_data_text}

```
(array([[ 0,  0,  0,  0,  1],
        [ 0,  0,  0,  1,  5],
        [ 0,  0,  1,  5,  2],
        [ 0,  1,  5,  2,  3],
        [ 1,  5,  2,  3,  6],
        [ 0,  0,  0,  0,  1],
        [ 0,  0,  0,  1,  7],
        [ 0,  0,  1,  7,  2],
        [ 0,  1,  7,  2,  3],
        [ 1,  7,  2,  3,  8],
        [ 0,  0,  0,  0,  9],
        [ 0,  0,  0,  9, 10],
        [ 0,  0,  9, 10, 11],
        [ 0,  9, 10, 11, 12],
        [ 9, 10, 11, 12, 13],
        [ 0,  0,  0,  0, 15],
        [ 0,  0,  0, 15, 16],
        [ 0,  0, 15, 16, 17],
        [ 0, 15, 16, 17, 18]]),
 array([ 5,  2,  3,  6,  4,  7,  2,  3,  8,  4, 10, 11, 12, 13, 14, 16, 17,
        18, 19]))
```



### label값의 원-핫 인코딩

<div class="in_prompt">
In&nbsp;[10]:
</div>

<div class="input_area" markdown="1">

```python
y_hot = to_categorical(y, num_classes=vocab_size)
y_hot
```

</div>

<div class="output_prompt">
Out&nbsp;[10]:
</div>




{:.output_data_text}

```
array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1.]], dtype=float32)
```



## 딥러닝

### 모델 정의, 학습

<div class="in_prompt">
In&nbsp;[11]:
</div>

<div class="input_area" markdown="1">

```python
from keras.models import Sequential
from keras.layers import Embedding, Dense, SimpleRNN
```

</div>

<div class="in_prompt">
In&nbsp;[12]:
</div>

<div class="input_area" markdown="1">

```python
model = Sequential([
    Embedding(vocab_size, 10), # vocab_size개의 단어를 10개의 특성으로 나타냄-다음 단어를 유추하기 위한 단어배치 Embedding (Word2Vec의 vector_size)
    SimpleRNN(32),
    Dense(vocab_size, activation='softmax')
])
model.summary()
```

</div>

<div class="output_prompt">
Out&nbsp;[12]:
</div>

{:.output_stream}

```
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, None, 10)          200       
                                                                 
 simple_rnn (SimpleRNN)      (None, 32)                1376      
                                                                 
 dense (Dense)               (None, 20)                660       
                                                                 
=================================================================
Total params: 2,236
Trainable params: 2,236
Non-trainable params: 0
_________________________________________________________________

```

<div class="in_prompt">
In&nbsp;[13]:
</div>

<div class="input_area" markdown="1">

```python
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
```

</div>

<div class="in_prompt">
In&nbsp;[14]:
</div>

<div class="input_area" markdown="1">

```python
history = model.fit(X, y, epochs=200, verbose=1)
```

</div>

<div class="output_prompt">
Out&nbsp;[14]:
</div>

{:.output_stream}

```
Epoch 1/200
1/1 [==============================] - 2s 2s/step - loss: 3.0064 - accuracy: 0.0000e+00
Epoch 2/200
1/1 [==============================] - 0s 3ms/step - loss: 2.9969 - accuracy: 0.0000e+00
Epoch 3/200
1/1 [==============================] - 0s 4ms/step - loss: 2.9875 - accuracy: 0.0526
Epoch 4/200
1/1 [==============================] - 0s 4ms/step - loss: 2.9782 - accuracy: 0.0526
Epoch 5/200
1/1 [==============================] - 0s 4ms/step - loss: 2.9690 - accuracy: 0.1053

...

Epoch 196/200
1/1 [==============================] - 0s 3ms/step - loss: 0.2667 - accuracy: 0.9474
Epoch 197/200
1/1 [==============================] - 0s 4ms/step - loss: 0.2640 - accuracy: 0.9474
Epoch 198/200
1/1 [==============================] - 0s 4ms/step - loss: 0.2612 - accuracy: 0.9474
Epoch 199/200
1/1 [==============================] - 0s 4ms/step - loss: 0.2585 - accuracy: 0.9474
Epoch 200/200
1/1 [==============================] - 0s 7ms/step - loss: 0.2559 - accuracy: 0.9474

```

<div class="in_prompt">
In&nbsp;[15]:
</div>

<div class="input_area" markdown="1">

```python
import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
```

</div>

<div class="output_prompt">
Out&nbsp;[15]:
</div>




{:.output_data_text}

```
[<matplotlib.lines.Line2D at 0x13d28bab280>]
```




    
![img]({{ '/assets/images/2023/01/12/img01.png' | relative_url }})
    


### 예측

<div class="in_prompt">
In&nbsp;[16]:
</div>

<div class="input_area" markdown="1">

```python
start_text = '이런 내 맘을 그댄' # expect: 아나요
encoded = tok.texts_to_sequences([start_text])[0]
encoded
```

</div>

<div class="output_prompt">
Out&nbsp;[16]:
</div>




{:.output_data_text}

```
[15, 16, 17, 18]
```



<div class="in_prompt">
In&nbsp;[17]:
</div>

<div class="input_area" markdown="1">

```python
pedded = pad_sequences([encoded], maxlen=max_len)
pedded
```

</div>

<div class="output_prompt">
Out&nbsp;[17]:
</div>




{:.output_data_text}

```
array([[ 0,  0, 15, 16, 17, 18]])
```



<div class="in_prompt">
In&nbsp;[18]:
</div>

<div class="input_area" markdown="1">

```python
res = model.predict(pedded, verbose=1)
res
```

</div>

<div class="output_prompt">
Out&nbsp;[18]:
</div>

{:.output_stream}

```
1/1 [==============================] - 0s 173ms/step

```




{:.output_data_text}

```
array([[2.4102295e-03, 1.5518441e-03, 8.5784467e-03, 1.9980723e-02,
        1.2252062e-02, 6.4658566e-04, 3.4566422e-04, 1.2817312e-04,
        7.5287861e-04, 3.0898005e-03, 1.6188608e-04, 2.6444297e-03,
        9.0402305e-02, 1.2629905e-02, 1.2723597e-04, 9.8875538e-04,
        7.9337502e-04, 2.9763640e-04, 1.4455848e-03, 8.4077239e-01]],
      dtype=float32)
```



<div class="in_prompt">
In&nbsp;[19]:
</div>

<div class="input_area" markdown="1">

```python
res_softmax = np.argmax(res, axis=1)
res_softmax
```

</div>

<div class="output_prompt">
Out&nbsp;[19]:
</div>




{:.output_data_text}

```
array([19], dtype=int64)
```



<div class="in_prompt">
In&nbsp;[20]:
</div>

<div class="input_area" markdown="1">

```python
start_text, *tok.sequences_to_texts([res_softmax])
```

</div>

<div class="output_prompt">
Out&nbsp;[20]:
</div>




{:.output_data_text}

```
('이런 내 맘을 그댄', '아나요')
```



<div class="in_prompt">
In&nbsp;[21]:
</div>

<div class="input_area" markdown="1">

```python
start_text = '저 별을 따다가 니' # expect: 귀에
encoded = tok.texts_to_sequences([start_text])[0]
pedded = pad_sequences([encoded], maxlen=max_len)
res = model.predict(pedded, verbose=1)
res_softmax = np.argmax(res, axis=1)
print(f'{start_text}:', *tok.sequences_to_texts([res_softmax]))
```

</div>

<div class="output_prompt">
Out&nbsp;[21]:
</div>

{:.output_stream}

```
1/1 [==============================] - 0s 15ms/step
저 별을 따다가 니: 귀에

```

<div class="in_prompt">
In&nbsp;[22]:
</div>

<div class="input_area" markdown="1">

```python
def word_predict(string):
    start_text = string
    encoded = tok.texts_to_sequences([start_text])[0]
    pedded = pad_sequences([encoded], maxlen=max_len)
    res = model.predict(pedded, verbose=1)
    res_softmax = np.argmax(res, axis=1)
    print(f'{start_text}:', *tok.sequences_to_texts([res_softmax]))
```

</div>

<div class="in_prompt">
In&nbsp;[23]:
</div>

<div class="input_area" markdown="1">

```python
word_predict('저 별을 따다가 니')
```

</div>

<div class="output_prompt">
Out&nbsp;[23]:
</div>

{:.output_stream}

```
1/1 [==============================] - 0s 21ms/step
저 별을 따다가 니: 귀에

```

단순히 어떤 단어가 뒤에 올지에 대한 특성이므로 Embedding으로 문맥을 학습하기는 제한적이다.
